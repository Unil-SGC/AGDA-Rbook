[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"Earth surface environmental processes exhibit distinctive characteristics, encompassing spatial temporal dimensions, along various attributes predictive variables.\nFurthermore, era Data Science, wealth available data rapid development analytical models emerged distinctive aspects realm Geospatial Data Analysis (GDA).\nCoupled uncertainty complexity issues, contribute making field research highly challenging.\ndomain GDA encompasses data exploration, manipulation, modelling, acquisition phase visualization interpretation results.\nMapping events located relate provides better understanding process studied.\nFinally, increasing volume geo-environmental data becomes accessible, demand experts domain growing rapidly, public research institutions well private companies.Defined first time Naur “science dealing data”, term Data Science evolved time around original concept “…converting data information knowledge” (IASC, 1977).\ndisciplines like Environmental Earth Sciences, Physical Geography, Humanities Social Sciences, use Data Science procedures emerging recently, proving extremely efficient deal complexity investigated process heterogeneity underlying data sources.\nleads cultural shift, moving scientists away individual working within research domain.\nIndeed, disciplinary boundaries permeable, pushing scientists open collaborate among decision makers investigation understanding real-world problems.\nModern earth environmental scientists need interact disciplines, apparently far domain.\nopenness increasingly important society struggles respond implication anthropogenic pressures different issues, natural hazards climate change, harmful impacts human activities biodiversity, water air quality, human health.target audience eBook master graduate (PhD) students Earth/Environmental Sciences, Biology Spatial Ecology, Physical Geography, equivalent disciplines.\nstrive empower students guiding theoretical knowledge hands-practical applications, enabling cultivate effective problem-solving skills.\nprimary focus delves applying Data Science methodologies understand analyze Earth’s surface environmental processes.\nscientific approaches related emerging discipline, ranging statistics, mathematics, geomatics computer science, often hard acquired.\nmaintaining strong emphasis rigorous mathematical statistical formalism methods presented, eBook primarily accentuates practical applications within realm GeosciencesThe book addressed intermediate advanced R users experience geospatial data great interest geocomputing.\nbasic knowledge fields, encourage explore links provided chapter, redirect useful documentation.eBook seeks provide audience :good understanding main practical concepts applied aspects GDA;Advanced tools designed proficiently navigate various techniques analysing spatial datasets geosciences.Specifically, methodologies outlined equip readers expertise various algorithms tailored analyzing complex geo-environmental datasets. knowledge empower extract valuable insights translate actionable decisions.Explored algorithms include:Geographically Weighted Summary Statistics exploratory data analyses visualization geographical variations statistical data distribution;Ripley’s K-function, Kernel Density Estimator, DBSCAN cluster detection mapping;Self-Organizing Maps example unsupervised machine learning approach data clustering visualization;Random Forest example supervised machine learning algorithm, applied classification.","code":""},{"path":"index.html","id":"prerequisites","chapter":"Preface","heading":"Prerequisites","text":"Knowledge basic statistics: methods descriptive statistics (measures central tendency dispersion); assess relationships variables; concepts correlation regression.Basic knowledge geomatics (GIS): basic operations raster vector datasets.R programming basics RStudio.","code":""},{"path":"index.html","id":"acknowledgements","chapter":"Preface","heading":"Acknowledgements","text":"case study presented chapter came different projects carried collaboration several colleagues, including master PhD students.\nproduced scientific papers duly cited bibliography.like thank following collaborators:\n- Mário Gonzalez Pereira Joana Parente, extensive fruitful collaboration investigating spatio-temporal distribution wildfires Portugal; one studies seeking investigate evolution forest fires, spatio-temporal point events smoothed density maps, integral part Chapters 2 3.\n- Stuart Lane Natan Micheletti, introducing fascinating world rock glacier research; notably, 3D-points clouds dataset analysed Chapter 4 acquired processed Natan PhD studies.\n- Axelle Bersier, meticulous work acquiring pre-processing Swiss national population census dataset, used exercise unsupervised learning Chapter 5.\n- Julien Riese, producing input dataset collaborating developing code allowing assess landslides susceptibility canton Vaud, main topic Chapter 6.\nJulien Axelle serve shining example highly successful master’s students pleasure supervising.","code":""},{"path":"introduction-to-r.html","id":"introduction-to-r","chapter":"1 Introduction to R","heading":"1 Introduction to R","text":"","code":""},{"path":"introduction-to-r.html","id":"r-language","chapter":"1 Introduction to R","heading":"1.1 R Language","text":"R complete programming language software environment statistical computing graphical representation.\npart GNU Project (free software, mass collaboration project), source code free available.\nfunctionalities can expanded importing packages.\ndetails R see https://www.r-project.org/.","code":""},{"path":"introduction-to-r.html","id":"r-packages","chapter":"1 Introduction to R","heading":"1.1.1 R Packages","text":"package file generally composed R scripts (e.g., functions).\noperation systems function “install.packages()” can used download install package automatically.\nOtherwise, package already installed R can loaded session using command .\nR, directories packages stored called “libraries”. terms “package” “library” sometimes used synonymously.\nexample, check list installed packeges, function can used.\nopen R Markdown document (.Rmd) program propose automatically install libraries listed .","code":""},{"path":"introduction-to-r.html","id":"some-tips","chapter":"1 Introduction to R","heading":"1.1.2 Some tips","text":"R case sensitive!Previously used command can recalled console using arrow keyboard.working directory default “C:/user/…/Documents”.\ncan found using command \ncan changed using command line \ncan found using command can changed using command line R Markdown: working directory evaluating R code chunks directory input document default.\naccess specific file sub-folder use “. /subfolder/file.ext”\naccess specific file -folder use “. . /upfolder/file.ext”\naccess specific file sub-folder use “. /subfolder/file.ext”access specific file -folder use “. . /upfolder/file.ext”","code":""},{"path":"introduction-to-r.html","id":"r-commands-online-resources","chapter":"1 Introduction to R","heading":"1.1.3 R Commands (online resources)","text":"Many table resuming main R commands can found online.\nuseful links:short list useful R commandsA short list useful R commandsTable Useful R commandsTable Useful R commandsBasic Commands Get Started RBasic Commands Get Started R","code":""},{"path":"introduction-to-r.html","id":"r-markdown","chapter":"1 Introduction to R","heading":"1.2 R Markdown","text":"R Markdown document :-)Markdown simple formatting syntax authoring HTML, PDF, MS Word documents.\nsimple easy use plain text language allowing combine R code, results data analysis (including plots tables), comments single nicely formatted reproducible document (like report, publication, thesis chapter web pages).Code lines organized code blocks, seeking solve specified tasks, referred “code chunk”.\ndetails using R Markdown see http://rmarkdown.rstudio.com.computing labs read explanatory paragraph running individual R code chunk, one one, interpret results.\nFinally, create personal document (usually PDF) rmarkdown, need Knit document.\nKnitting document simply means taking text code creating nicely formatted document.","code":""},{"path":"introduction-to-r.html","id":"data-type-in-computational-analysis","chapter":"1 Introduction to R","heading":"1.3 Data type in computational analysis","text":"","code":""},{"path":"introduction-to-r.html","id":"variables","chapter":"1 Introduction to R","heading":"1.3.1 Variables","text":"Variables used store values computer program.\nValues can numbers (real complex), words (string), matrices, even tables.fundamental atomic data R Programming can :integer: number without decimalsnumeric: number decimals (float double depending precision)character: string, labelfactors: label limited number categorieslogical: true/false\nFigure 1.1: Data Types R\n","code":""},{"path":"introduction-to-r.html","id":"data-structure-in-r","chapter":"1 Introduction to R","heading":"1.3.2 Data structure in R","text":"R’s base data structures can organised dimensionality (1d, 2d, nd) whether homogeneous (contents must type) heterogeneous (contents can different types).gives rise four data structures often used data analysis:\nFigure 1.2: Data structures R\nVector one-dimensional structure winch can contain object one type : numerical (integer double), character, logical.Matrix two-dimensional structure winch can contain object one type .\nfunction can used construct matrices specific dimensions.","code":"\n# Investigate vector's types:\n\nv1 <- c(0.5, 0.7); v1; typeof(v1)\n#> [1] 0.5 0.7\n#> [1] \"double\"\n\nv2 <-c(1:10); v2; typeof(v2)\n#>  [1]  1  2  3  4  5  6  7  8  9 10\n#> [1] \"integer\"\n\nv3 <- c(TRUE, FALSE); v3; typeof(v3)\n#> [1]  TRUE FALSE\n#> [1] \"logical\"\n\nv4 <- c(\"Swiss\", \"Itay\", \"France\", \"Germany\"); v4; typeof(v4)\n#> [1] \"Swiss\"   \"Itay\"    \"France\"  \"Germany\"\n#> [1] \"character\"\n#Create a sequence from 0 to 5 with a step of 0.5:\n\nv5 <- seq(1, 5, by=0.5); v5; typeof(v5)\n#> [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\n#> [1] \"double\"\n\nlength(v5)\n#> [1] 9\n\nsummary(v5)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>       1       2       3       3       4       5\n#Extract the third element of the vector\nv5[3]\n#> [1] 2\n\n#Exclude the third element from the vector and save as new vector\nv5[-3]\n#> [1] 1.0 1.5 2.5 3.0 3.5 4.0 4.5 5.0\nw5<-v5[-3]; w5\n#> [1] 1.0 1.5 2.5 3.0 3.5 4.0 4.5 5.0\n\n# Matrix of elements equal to \"zero\" and dimension 2x5 \nm1<-matrix(0,2,5); m1  #(two rows by five columns)\n#>      [,1] [,2] [,3] [,4] [,5]\n#> [1,]    0    0    0    0    0\n#> [2,]    0    0    0    0    0\n\n# Matrix of integer elements (1 to 12, 3x4) \nm2<-matrix(1:12, 3,4); m2 \n#>      [,1] [,2] [,3] [,4]\n#> [1,]    1    4    7   10\n#> [2,]    2    5    8   11\n#> [3,]    3    6    9   12\n\n# Extract the second row\nm2[2, ]\n#> [1]  2  5  8 11\n# Extract the third column\nm2[,3]\n#> [1] 7 8 9\n# Extract the the second element of the third column\nm2[2,3]\n#> [1] 8"},{"path":"introduction-to-r.html","id":"data-frame","chapter":"1 Introduction to R","heading":"1.3.3 Data Frame","text":"data frame allows collect data different type.\nelements must length.list flexible structure since can contain variables different types lengths.\nNevertheless, preferred structure statistical analyses computation data frame.good practice explore data frame performing computation data.\ncan simply accomplished using commands explore structure data display summary statistics quickly summarize data.\nnumerical vectors command can used plot basic histogram given values.","code":"\n# Create the vectors with the variables\n\ncities <- c(\"Berlin\", \"New York\", \"Paris\", \"Tokyo\")\narea <- c(892, 1214, 105, 2188)\npopulation <- c(3.4, 8.1, 2.1, 12.9)\ncontinent <- c(\"Europe\", \"Norh America\", \"Europe\", \"Asia\")\n# Concatenate the vectors into a new data frame\ndf1 <- data.frame(cities, area, population, continent)\ndf1\n#>     cities area population    continent\n#> 1   Berlin  892        3.4       Europe\n#> 2 New York 1214        8.1 Norh America\n#> 3    Paris  105        2.1       Europe\n#> 4    Tokyo 2188       12.9         Asia\n\n#Add a column (e.g., language spoken) using the command \"cbind\"\ndf2 <- cbind (df1, \"Language\" = c (\"German\", \"English\", \"Freanch\", \"Japanese\"))\ndf2\n#>     cities area population    continent Language\n#> 1   Berlin  892        3.4       Europe   German\n#> 2 New York 1214        8.1 Norh America  English\n#> 3    Paris  105        2.1       Europe  Freanch\n#> 4    Tokyo 2188       12.9         Asia Japanese\n#Explore the data frame\nstr(df2) # see the structure\n#> 'data.frame':    4 obs. of  5 variables:\n#>  $ cities    : chr  \"Berlin\" \"New York\" \"Paris\" \"Tokyo\"\n#>  $ area      : num  892 1214 105 2188\n#>  $ population: num  3.4 8.1 2.1 12.9\n#>  $ continent : chr  \"Europe\" \"Norh America\" \"Europe\" \"Asia\"\n#>  $ Language  : chr  \"German\" \"English\" \"Freanch\" \"Japanese\"\nsummary(df2) # compute basic statistics\n#>     cities               area          population    \n#>  Length:4           Min.   : 105.0   Min.   : 2.100  \n#>  Class :character   1st Qu.: 695.2   1st Qu.: 3.075  \n#>  Mode  :character   Median :1053.0   Median : 5.750  \n#>                     Mean   :1099.8   Mean   : 6.625  \n#>                     3rd Qu.:1457.5   3rd Qu.: 9.300  \n#>                     Max.   :2188.0   Max.   :12.900  \n#>   continent           Language        \n#>  Length:4           Length:4          \n#>  Class :character   Class :character  \n#>  Mode  :character   Mode  :character  \n#>                                       \n#>                                       \n#> \n\n# Use the symbol \"$\" to address a particular column\npop<-(df2$population)\npop\n#> [1]  3.4  8.1  2.1 12.9\nhist(pop) # plot the histogram"},{"path":"geographically-weighted-summary-statistics.html","id":"geographically-weighted-summary-statistics","chapter":"2 Geographically Weighted Summary Statistics","heading":"2 Geographically Weighted Summary Statistics","text":"","code":""},{"path":"geographically-weighted-summary-statistics.html","id":"introduction","chapter":"2 Geographically Weighted Summary Statistics","heading":"2.1 Introduction","text":"fire management, crucial investigate fires occurred frequently distinguish small large fires.\nkey information understand ignition factors planning strategies reduce forest fires, control manage ignition sources, identify areas risk.Despite availability forest fires spatio-temporal inventories, evident extract information pattern distribution simply looking original arrangement mapped burnt areas.\nend, Geographically Weighed Summary Statistics (GWSS) can computed, assumption burned areas generally follow geographic trend.compute GW local means, GW local standard deviation GW localized skewness burned areas continental Portugal, registered period 1990-2013.\napplication inspired work of1","code":""},{"path":"geographically-weighted-summary-statistics.html","id":"the-overall-methodology","chapter":"2 Geographically Weighted Summary Statistics","heading":"2.2 The overall methodology","text":"Summary statistics include number measures can used summarize set observations, important measures central tendency (arithmetic mean, median mode) measures dispersion around mean (variance standard deviation).\naddition, measures skewness kurtosis descriptors shape probability distribution function, former indicating asymmetry latter peakedness/tailedness curve.case spatial data, global statistical descriptors may vary one region another, values may affected local environmental socio-economic factors.\ncase, appropriately localized calibration can provide better description observed values.\nOne way achieve goal weight statistical measures given quantitative variable based geographical location.introduce method proposed by2 implemented function GWSS presented .3\nevaluation geographically weighted summary statistics obtained computing summary small area around geolocalized punctual observation, using kernel density estimation technique (KDE).4\nKDE estimated point, taking account influence points falling within area, increasing weight towards center, corresponding point location.\nsurface summary statistic thus obtained.","code":""},{"path":"geographically-weighted-summary-statistics.html","id":"forest-fires-dataset","chapter":"2 Geographically Weighted Summary Statistics","heading":"2.3 Forest fires dataset","text":"Forest fires inventories indicating location, starting date related variables, cause ignition size area burned, broadly available different degree accuracy different countries.present study, consider Portuguese National Mapping Burnt Areas (NMBA 2016), freely available website Institute Conservation Nature Forests (ICNF).\nlong spatio-temporal dataset (1975) resulting processing satellite images acquired year end summer season.\nRow data consist records observed fire scars.\nburned areas estimated using image classification techniques, compared ground data resolve discrepancies.\nPolygons converted point shapefile, point represent centroid burned areas, size burned areas starting date fires events given attributes.\nwork, consistency reasons, consider fires occurred 1990 2013 burned area 5 hectares.\n.\nFigure 2.1: Total annual number forest fire events, expressed thousands square metres\n","code":""},{"path":"geographically-weighted-summary-statistics.html","id":"load-the-libraries","chapter":"2 Geographically Weighted Summary Statistics","heading":"2.3.1 Load the libraries","text":"First load following libraries:splancs: display analysis spatial point pattern dataGWmodel: techniques particular branch spatial statistics, termed geographically-weighted (GW) modelssf: support simple features, standardized way encode spatial vector dataggplot2: system ‘declaratively’ creating graphicssp: classes methods spatial data","code":"#>  [1] \"ggplot2\"    \"sf\"         \"GWmodel\"    \"Rcpp\"      \n#>  [5] \"robustbase\" \"splancs\"    \"sp\"         \"stats\"     \n#>  [9] \"graphics\"   \"grDevices\"  \"utils\"      \"datasets\"  \n#> [13] \"methods\"    \"base\""},{"path":"geographically-weighted-summary-statistics.html","id":"import-the-portuguese-forest-fire-dataset","chapter":"2 Geographically Weighted Summary Statistics","heading":"2.3.2 Import the Portuguese forest fire dataset","text":"section load geodata representing dataset forest fires occurred continental Portuguese area period 1990-2013.\nalso load boundaries study area.\nstart exploring datasets using mainly visual tools (plotting histogram).can explore dataset using different tools exploratory data analyses.\nstart visualizing databases.\nGIS environment, correspond attribute table vector punctual feature.can plot histogram events distribution based variable “Area_ha” (size hectares burned area).\nSince power low distribution, better understanding ’s recommended transform data using logarithmic scale.\nUsing Log10 can easily evaluate frequency distribution burned areas.","code":""},{"path":"geographically-weighted-summary-statistics.html","id":"forest-fires-spatial-distribution","chapter":"2 Geographically Weighted Summary Statistics","heading":"2.3.3 Forest fires spatial distribution","text":"better understanding phenomenon, can group events according size burned area.\nBased frequency distribution burned areas, following three classes can defined:Small fires: less 15 haSmall fires: less 15 haMedium fires: 15 ha 100 haMedium fires: 15 ha 100 haLarge fires: bigger 100 haLarge fires: bigger 100 haPlotting forest fires events using different colors, based size burned areas, can simplify understanding pattern distribution, knowing fires different size normally different drivers.","code":""},{"path":"geographically-weighted-summary-statistics.html","id":"compute-the-geographically-whited-statistics","chapter":"2 Geographically Weighted Summary Statistics","heading":"2.4 Compute the geographically whited statistics","text":"exploratory data analysis performed , seems simple plotting forest fires events based spatial distribution, even classified based size, can really help understand behaviors.\nface huge number events variable using characterize (.e., size burned area) heterogeneous.\naim, can compute basic robust GWSS plot data accordingly.GWSS includes geographically weighted means, standard deviations skweness.\ncan see R Documentation (command: , data manipulations necessary transform forest fires dataset compatible data frame format.GWSS parameters:summarize data based size burned area (vars).summarize data based size burned area (vars).use adaptive kernel bandwidth (bw) corresponds number (100 case) nearest neighbors (.e. adaptive distance).use adaptive kernel bandwidth (bw) corresponds number (100 case) nearest neighbors (.e. adaptive distance).keep default values parameters.keep default values parameters.","code":""},{"path":"geographically-weighted-summary-statistics.html","id":"look-at-the-results","chapter":"2 Geographically Weighted Summary Statistics","heading":"2.4.1 Look at the results","text":"resulting object (FFgwss) number components.\nimportant one spatial data frame containing results local summary statistics data point location, stored FFgwss$SDF (spatial DataFrame).","code":""},{"path":"geographically-weighted-summary-statistics.html","id":"gwss-maps","chapter":"2 Geographically Weighted Summary Statistics","heading":"2.4.2 GWSS maps","text":"produce map local geographically weighted summary statistic choice, firstly need enter small R function definition.\njust short R program draw map: can think command tells R draw map (see Geographically Weighted Summary Statistics (https://rpubs.com/chrisbrunsdon/99667) details).\nadvantage defining function entire map can now drawn using single command variable, rather repeat steps time.\ndefine intervals classification used Jenks natural breaks classification method (textcolor{red}{style=“fisher”}).Finally function called entering:","code":"quick.map(gwss.object,variable.name,legend.title,main.map.title)"},{"path":"geographically-weighted-summary-statistics.html","id":"conclusions-and-further-analyses","chapter":"2 Geographically Weighted Summary Statistics","heading":"2.5 Conclusions and further analyses","text":"practical computer lab allowed familiarize GWSS, proposed application geographically weighted summary statistics.\nmethod allowed us explore average burned area vary locally Continental Portugal period 1990-2013.global Geographically Weighted (GW) means informs local average value burned area, based neighboring events occurred given period.\nSimilarly, may compute GW standard deviation see extent size burned area spread around mean.\nFinally can compute GW skewness measure symmetry distribution: positively skewed distribution means higher number data points low values, mean value lower median; contrary negatively skewed distribution.sure everything perfectly clear , propose answer following questions discuss answers participants course directly teacher.pattern distribution GW-means burned area Portugal investigated periods?pattern distribution GW-means burned area Portugal investigated periods?GW-standard deviation follows pattern?\ncan interpret two pattern terms burned area characterization?GW-standard deviation follows pattern?\ncan interpret two pattern terms burned area characterization?GW-skewness positive values everywhere: means?\nvalues suggest distribution burned areas, terms size, around local means?GW-skewness positive values everywhere: means?\nvalues suggest distribution burned areas, terms size, around local means?can applications GWSS geo-environmental data?\nwords, can imagine geo-environmental dataset can analysed using GWSS?can applications GWSS geo-environmental data?\nwords, can imagine geo-environmental dataset can analysed using GWSS?can finally play code try run using different numbers nearest neighbors (bw=x) comparing results.\nNB: rename original pdf avoid overwriting .\naddition, pdf name saved destination folder open, receive error message, close Knitting.can finally play code try run using different numbers nearest neighbors (bw=x) comparing results.NB: rename original pdf avoid overwriting .\naddition, pdf name saved destination folder open, receive error message, close Knitting.","code":""},{"path":"kernel-density-estimator.html","id":"kernel-density-estimator","chapter":"3 Kernel Density Estimator","heading":"3 Kernel Density Estimator","text":"","code":""},{"path":"kernel-density-estimator.html","id":"introduction-1","chapter":"3 Kernel Density Estimator","heading":"3.1 Introduction","text":"configuration forest fires across space time presents complex pattern significantly affects forest environment adjacent human developments.\nStatistical techniques designed spatio-temporal random points can utilized identify structure, recognize hot-spots vulnerable areas, address policy makers prevention forecasting measures.practical computer lab consider case study “Geographically Weighted Summary Statistics” lab.\nmain objective reveal space time act independently whether neighboring events also closer time, interacting generate spatio-temporal clusters.\nattribute consider achieve goal starting date fires events.\naccount different geographical distribution fires Portugal, evets occurred Norther Southern area modeled separately.details input dataset, please refer GWSS lab documentation.","code":""},{"path":"kernel-density-estimator.html","id":"method","chapter":"3 Kernel Density Estimator","heading":"3.2 Method","text":"detect spatio-temporal clusters forest fires, use following statistical methods:Ripley’s K- function, test space–time interaction spatial attraction/dependency fires different size.Ripley’s K- function, test space–time interaction spatial attraction/dependency fires different size.kernel density estimator, allowing elaborating smoothed density surfaces representing fires -densities.kernel density estimator, allowing elaborating smoothed density surfaces representing fires -densities.provide short description methods.\ndetails can found Tonini et al.5","code":""},{"path":"kernel-density-estimator.html","id":"ripleys-k-function","chapter":"3 Kernel Density Estimator","heading":"3.2.1 Ripley’s K-function","text":"Ripley’s K-function allows inferring spatial randomness mapped punctual events.\nlargely applied environmental studies analyse pattern distribution spatial point process.\noriginal spatial univariate K-function \\(K(s)\\) defined ratio expected number \\(E\\) point events falling certain distance \\(r\\) arbitrary event intensity \\(\\lambda\\) spatial point process, last corresponding average number points per unit area.complete spatial randomness, assumes independence among events, \\(K(s)\\) equal area circle around target event distance’s value.\nfollows events spatially clustered within range distances \\(K(s)\\) assumes vales higher area, spatially dispersed lower values.\ntemporal K-function \\(K(t)\\) defined way spatial case, time-based intensity time length replacing spatial parameters.spatio-temporal K-function, \\(K(s,t)\\) can considered bivariate function space time represent two variables equation.\ndefined number events occurring within distance \\(r\\) time \\(t\\) arbitrary event.","code":""},{"path":"kernel-density-estimator.html","id":"spatio-temporal-interaction","chapter":"3 Kernel Density Estimator","heading":"3.2.1.1 Spatio-temporal interaction","text":"space–time interaction, \\(K(s,t)\\) equal product purely spatial purely temporal K-function.\nInversely, space time interact generating clusters, difference two values positive [\\(D(s,t)=K(s,t)-K(s)*K(t)\\)].Thus, can use perspective 3D-plot function \\(D(s,t)\\) obtain first diagnostic space–time clustering: positive values indicate interaction two variables well-detectable spatio-temporal scale.","code":""},{"path":"kernel-density-estimator.html","id":"kernel-density-estimator-1","chapter":"3 Kernel Density Estimator","heading":"3.2.2 Kernel density estimator","text":"Kernel Density Estimator (KDE) non-parametric descriptor tool widely applied GIS-science elaborate smoothed density surfaces spatial variables.\nkernel function \\(K\\) allows weighing contribution event, based relative distance neighbouring target.\nparameter \\(h\\), called bandwidth, controls smoothness estimated kernel density.\nFinally, kernel density function \\(f_h(x)\\) estimated summing kernel functions \\(K\\) computed point location \\(x\\) dividing result total number events (\\(n\\)): \\[f_h(x) = \\frac{1}{nh}\\sum_{=j}^{n}K(\\frac{x-x_i}{h})\\]time extension kernel density estimator (Tomoki Nakaya Keiji Yano6) allows compute three-dimensional kernel density estimator includes spatio-temporal dimensions.present case study apply quadratic weighting kernel function, approximation Gaussian kernel.\nRegarding bandwidth’s value, prose consider results spatio-temporal K-function indicator.\nIndeed, distance values showing maximum cluster behavior displayed perspective \\(D(s,t)\\) plot can attributed \\(h\\)-value, minimizing problem - -smoothing due arbitrary choice bandwidth.","code":""},{"path":"kernel-density-estimator.html","id":"load-the-libraries-1","chapter":"3 Kernel Density Estimator","heading":"3.3 Load the libraries","text":"Fist load following libraries:splancs: display analysis spatial point pattern data.sf: Support simple features, standardized way encode spatial vector dataggplot2: system ‘declaratively’ creating graphicssp: Classes methods spatial dataspatstat: comprehensive open-source toolbox analyzing Spatial Point Patterns","code":"\n\nlibrary(splancs)\nlibrary(spatstat)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(sp)\n\n(.packages())\n#>  [1] \"ggplot2\"          \"sf\"              \n#>  [3] \"spatstat\"         \"spatstat.linnet\" \n#>  [5] \"spatstat.model\"   \"rpart\"           \n#>  [7] \"spatstat.explore\" \"nlme\"            \n#>  [9] \"spatstat.random\"  \"spatstat.geom\"   \n#> [11] \"spatstat.data\"    \"splancs\"         \n#> [13] \"sp\"               \"stats\"           \n#> [15] \"graphics\"         \"grDevices\"       \n#> [17] \"utils\"            \"datasets\"        \n#> [19] \"methods\"          \"base\""},{"path":"kernel-density-estimator.html","id":"import-the-portuguese-forest-fire-dataset-1","chapter":"3 Kernel Density Estimator","heading":"3.4 Import the Portuguese forest fire dataset","text":"section load geodata representing dataset Forest Fires (FF) occurred continental Portuguese area period 1990-2013.\nalso load boundaries study area.\nstart exploring datasets using mainly visual tools (plotting histogram).better understanding phenomenon, can group events according size burnt area incidence northern southern part continental Portugal.","code":"\n\n# Import Portugal boundary \nPortugal <- st_read(\"data/Lab02/Portugal.shp\") # entire area\n\nPortN<- st_read(\"data/Lab02/Porto_North.shp\") # northern area \nPortS<- st_read(\"data/Lab02/Porto_South.shp\") # southern\n\n# Import the forest fires dataset\n\nFF<-st_read(\"data/Lab02/ForestFires.shp\") # entire area\nFFN<-st_read(\"data/Lab02/FF_North.shp\") #entire area\nFFS<-st_read(\"data/Lab02/FF_South.shp\") # Northern area\n\n# Import the shapefile of the Tagus river\nriver<-st_read(\"data/Lab02/Rio_Tajo.shp\")\n\nsummary(FF$Area_ha)# summary statistics\nhist(FF$Area_ha)\n\nhist(log10(FF$Area_ha)) # see the lab GWSS for more details "},{"path":"kernel-density-estimator.html","id":"ff-subsets-based-on-the-burned-areas-size","chapter":"3 Kernel Density Estimator","heading":"3.4.1 FF-subsets based on the burned areas’ size","text":"Remember fires different size can induced different drivers.\nThus, following, investigate global cluster behavior forest fires Portugal considering three subset separately.\nseen lab GWSS, based frequency distribution burned areas, following three classes can defined:* Small fires: less 15 ha* Medium fires: 15 ha 100 ha* Large fires: bigger 100 ha","code":"\n\nSF=(subset(FF, Area_ha <=15)) #create a sub-set including only small fires. \n\nsummary(SF)\n#>       Year         Area_ha             X         \n#>  Min.   :1990   Min.   : 5.000   Min.   : 82761  \n#>  1st Qu.:1997   1st Qu.: 6.624   1st Qu.:182972  \n#>  Median :2001   Median : 8.569   Median :213159  \n#>  Mean   :2002   Mean   : 9.041   Mean   :219789  \n#>  3rd Qu.:2008   3rd Qu.:11.219   3rd Qu.:259054  \n#>  Max.   :2013   Max.   :15.000   Max.   :361492  \n#>        Y                   geometry    \n#>  Min.   :  7772   POINT        :10902  \n#>  1st Qu.:403849   epsg:NA      :    0  \n#>  Median :472892   +proj=tmer...:    0  \n#>  Mean   :440692                        \n#>  3rd Qu.:518384                        \n#>  Max.   :573192\n\n# This is to save the plot \npSF <- ggplot ()+\n  geom_sf(data=Portugal)+\n  geom_sf(data=SF, size=0.5, col=\"yellow\")+\n ggtitle(\"Small fires\") +\n coord_sf()\n\nMF=(subset(FF, Area_ha >15 & Area_ha <=100)) #create a sub-set including only medium fires. \n\nsummary(MF)\n#>       Year         Area_ha            X         \n#>  Min.   :1990   Min.   :15.00   Min.   : 82127  \n#>  1st Qu.:1997   1st Qu.:21.18   1st Qu.:186031  \n#>  Median :2001   Median :30.89   Median :219459  \n#>  Mean   :2002   Mean   :38.55   Mean   :223915  \n#>  3rd Qu.:2007   3rd Qu.:51.06   3rd Qu.:264349  \n#>  Max.   :2013   Max.   :99.98   Max.   :359535  \n#>        Y                   geometry    \n#>  Min.   :  7084   POINT        :12070  \n#>  1st Qu.:403864   epsg:NA      :    0  \n#>  Median :468503   +proj=tmer...:    0  \n#>  Mean   :439442                        \n#>  3rd Qu.:518248                        \n#>  Max.   :572488\n\n# This is to save the plot \npMF <- ggplot ()+\n  geom_sf(data=Portugal)+\n  geom_sf(data=MF, size=0.5, col=\"orange\")+\n ggtitle(\"Midium fires\") +\n coord_sf()\n\nLF=(subset(FF, Area_ha >100)) #create a sub-set including only large fires.\n\nsummary (LF)\n#>       Year         Area_ha              X         \n#>  Min.   :1990   Min.   :  100.0   Min.   : 83472  \n#>  1st Qu.:1996   1st Qu.:  139.6   1st Qu.:193831  \n#>  Median :2002   Median :  220.7   Median :228709  \n#>  Mean   :2001   Mean   :  548.3   Mean   :229278  \n#>  3rd Qu.:2006   3rd Qu.:  447.9   3rd Qu.:266146  \n#>  Max.   :2013   Max.   :66070.6   Max.   :357933  \n#>        Y                   geometry   \n#>  Min.   :  8840   POINT        :4301  \n#>  1st Qu.:383164   epsg:NA      :   0  \n#>  Median :445272   +proj=tmer...:   0  \n#>  Mean   :423441                       \n#>  3rd Qu.:500321                       \n#>  Max.   :571984\n\n# This is to save the plot \npLF <- ggplot ()+\n  geom_sf(data=Portugal)+\n  geom_sf(data=LF, size=0.5, col=\"red\")+\n ggtitle(\"Large fires\") +\n coord_sf()\n\n# Arrange the three spatial maps side by side\n\ninstall.packages('patchwork', repos = \"http://cran.us.r-project.org\") \n#> \n#> The downloaded binary packages are in\n#>  /var/folders/hf/q_2qq0tn1dngf4r3nzfhj3xh0000gn/T//RtmphpzSNI/downloaded_packages\nlibrary(patchwork) # Allow to  combine separate ggplots into the same graphic\n\npSF+pMF+pLF"},{"path":"kernel-density-estimator.html","id":"ff-subsets-based-on-their-geographical-distribution","chapter":"3 Kernel Density Estimator","heading":"3.4.2 FF-subsets based on their geographical distribution","text":"continental Portugal, northern half country (Tagus River) characterized predominance forest semi-natural areas, development main cities sub-urban ares intermingled wild land, Northern area highly prone forest fires.\nhand, southern half country dominated agricultural areas mixed broad-leaved forest concentrated near south-west coast, makes less affected forest fires.\nreason consider two areas separately.","code":"\n\n# Plot the map with all the objects\nggplot ()+\n  geom_sf(data=Portugal)+\n  geom_sf(data=river, col=\"blue\", size=2) +\n  geom_sf(data=FFN, size=0.3, col=\"red\") +\n  geom_sf(data=FFS, size=0.3, col=\"orange\") +\n  ggtitle(\"Forest foirest in the northern and souther area\") +\n  theme(plot.title=element_text(hjust=0.5)) +\n coord_sf()"},{"path":"kernel-density-estimator.html","id":"the-space-time-k-function","chapter":"3 Kernel Density Estimator","heading":"3.5 The space-time K-function","text":"","code":""},{"path":"kernel-density-estimator.html","id":"extract-time-and-pts-object","chapter":"3 Kernel Density Estimator","heading":"3.5.1 Extract Time and PTS object","text":"function stkhat, included library spancs, allows compute space-time K-function.\ncan see R Documentation (command: help(stkhat)), data manipulations necessary transform input data compatible data frame format.Namely user needs specify:pts: input forest fires dataset, coordinates geolocalize event.pts: input forest fires dataset, coordinates geolocalize event.times: vector times, defined starting date ignition.times: vector times, defined starting date ignition.poly: polygon class matrix enclosing input dataset (pts)poly: polygon class matrix enclosing input dataset (pts)s tm: vector spatial (s) vector temporal (tm) distances analysis.s tm: vector spatial (s) vector temporal (tm) distances analysis.","code":"\n\n# Extract \"pts\" (divided by 1000 to compute in Km)\nFFN_pts<-as.points(FFN$X/1000,FFN$Y/1000) \nFFN_times<-FFN$Year # extract \"times\"\n\n# Extract the coordinates (in Km):\nPTN_xy<-st_coordinates(PortN$geometry/1000) \n\n# Define the matrix with the set of bounding points (\"poly\") enclosing the input dataset:\nFFN_poly<-PTN_xy[, -c(3,4)] "},{"path":"kernel-density-estimator.html","id":"compute-the-space-time-k-function","chapter":"3 Kernel Density Estimator","heading":"3.5.2 Compute the space-time K-function","text":"Compute space-time K-function forest fires northern area.\nSince computation can take long time (20 mints), propose load directly output R object provided (STK_North_10y).\ngeneral code also provided, preceded hashtag, treated command: remove # wish evaluate code.consider subset forest fires event occurred period 2001-2010.consider subset forest fires event occurred period 2001-2010.parameters s t defined follows: (s) kilometer distance ten kilometers; (tm) year five years.parameters s t defined follows: (s) kilometer distance ten kilometers; (tm) year five years.NB: wish run code, remove # make work","code":"\n\n# Open stkhat documentation\nhelp(stkhat)\nlibrary(readr)\n\nSTK_North_10y <- readRDS(\"data/Lab02/STK_North_10y.RData\")\n\nstr(STK_North_10y)\n#> List of 5\n#>  $ s  : num [1:11] 0 1 2 3 4 5 6 7 8 9 ...\n#>  $ t  : num [1:6] 0 1 2 3 4 5\n#>  $ ks : num [1:11] 1.61e-04 9.03 2.98e+01 6.18e+01 1.05e+02 ...\n#>  $ kt : num [1:6] 0.814 2.29 3.596 4.827 6.085 ...\n#>  $ kst: num [1:11, 1:6] 1.45e-03 1.06e+01 3.75e+01 7.47e+01 1.22e+02 ..."},{"path":"kernel-density-estimator.html","id":"assess-the-space-time-clustering-behavior","chapter":"3 Kernel Density Estimator","heading":"3.5.3 Assess the space-time clustering behavior","text":"following section explore plot values three components produced outputs function stkhat: spatial K-function (ks), temporal K-function (kt); space-time K-function (kst).Finally plot perspective 3D-plot \\(D(s,t)\\) evaluate space–time clustering behavior forest fires present case study.\nmultifaceted shape function, along spatial temporal dimension, can help identify peaks clustering.\nFinally, corresponding values can attributed bandwidth kernel density estimators allowing elaborate smoothed density maps last step lab.","code":""},{"path":"kernel-density-estimator.html","id":"plot-the-stkhat-outputs","chapter":"3 Kernel Density Estimator","heading":"3.5.3.1 Plot the stkhat outputs","text":"Plot purely spatial purely temporal K function.Plot space-time D-plot","code":"\n\n# Plot of the purely spatial K function\nplot(STK_North_10y$s, STK_North_10y$ks, type=\"l\", xlab=\"distance\", ylab=\"Estimated Ks\", main=\"Spatial K function\")\nlines(STK_North_10y$s, pi*STK_North_10y$s^2, type=\"l\", col=\"red\")\n\n# Plot of the purely temporal K-function\nplot(STK_North_10y$t, STK_North_10y$kt, type=\"l\", xlab=\"time\", ylab=\"Estimated Kt\", main=\"Temporal K function\")\nlines(STK_North_10y$t, 2*STK_North_10y$t, type=\"l\", col=\"red\")\n\n# Define the function: D(s,t)=K(s,t)-[K(s)*K(t)]\n\nDplot <- function (stkhat, Dzero = FALSE, main=TRUE)  {\n\n  oprod <- outer(stkhat$ks, stkhat$kt)\n    st.D <- stkhat$kst - oprod\n        persp(stkhat$s, stkhat$t, st.D, xlab = \"Distance (Km)\", ylab = \"Time (years)\",\n            zlab = \"D(s,t)\", expand = 0.5, ticktype = \"detailed\",\n            theta = -45, shade = 0.75, cex = 0.7, ltheta=120, col=\"cyan1\", font.lab=2)\n           }\n\nDplot(STK_North_10y)\ntitle(\"Dplot Nothern Fires\")"},{"path":"kernel-density-estimator.html","id":"run-the-space-time-kernel-density-estimator","chapter":"3 Kernel Density Estimator","heading":"3.5.3.2 Run the space-time kernel density estimator","text":"multifaceted shape D-Plot identify peaks clustering time value 3 year.\nspece, events clustered every distance, case use maximum values (10 km).\ntwo values attributed bandwidth kernel density estimators allowing elaborate smoothed density maps.","code":"\n# Open the help to analyse the parameter of this kernel function: \nhelp(kernel3d)\n# Run the function\nKDE_FFN<-kernel3d(FFN_pts, FFN_times, seq(80, 362, 1), seq(180, 580, 1), seq(1990,2013,1), 10, 3)\nsummary(KDE_FFN$v)\nhist(log10(KDE_FFN$v))\n\nmin(KDE_FFN$v[KDE_FFN$v>0]) #check the lower non-zero value \n#> [1] 2.893333e-12\n\n# Create quantile clssification \nQ<-quantile(KDE_FFN$v, seq(0.5,1,0.05))\nQ\n#>          50%          55%          60%          65% \n#> 0.0002194741 0.0097073381 0.0263666531 0.0478856382 \n#>          70%          75%          80%          85% \n#> 0.0748979448 0.1093260662 0.1560709189 0.2210177893 \n#>          90%          95%         100% \n#> 0.3232627322 0.5162620818 2.1627084390\n\n# Create a blue/red palette\npal<-colorRampPalette(c(\"grey\",\"blue\",\"green\", \"yellow\",\"orange\", \"red\" ))\ncolsR<-pal(length(Q)-1)\n\n# Display classes\npie(Q, clockwise=TRUE, labels=round(Q, digits=2), border=\"white\", col=colsR)\n\n# Plot KDE maps for selected years\noldpar<-par(mfrow=c(5,5), mar=c(1,1,1,1))\nfor (i in 1:24){\n (image(seq(80, 362, 1), seq(180, 580, 1), KDE_FFN$v[,,i], asp=1, xlab=\"\", ylab=\"\", main=1989+i, breaks=Q, col=colsR))\n}"},{"path":"kernel-density-estimator.html","id":"conclusions-and-further-analyses-1","chapter":"3 Kernel Density Estimator","heading":"3.6 Conclusions and further analyses","text":"practical computer lab allowed familiarize assement global cluster behavior hazardous events, achieved using Ripley’s k-function.\naddition, learned smoothed density maps can elaborated punctual events, namely using kernel density estimator.\nspatial temporal dimension considered case.method allowed us explore density distribution forest fires events Continental Portugal period 1990-2013.\nnorthern half country, hot spots present almost investigated years, higher concentration northern areas.sure everything perfectly clear , propose lab , using time another subset chosen among forest fires Southern area, small, medium large forest fires dataset.whatever dataset going use, try answer following questions.\nreading paper Tonini et al.7, inspired lab, can help task.spatial temporal distance can observe peak clustering?spatial temporal distance can observe peak clustering?Describe spatio-temporal density distribution forest fires events study area.Describe spatio-temporal density distribution forest fires events study area.","code":""},{"path":"density-based-clustering-algorithm.html","id":"density-based-clustering-algorithm","chapter":"4 Density-based Clustering Algorithm","heading":"4 Density-based Clustering Algorithm","text":"","code":""},{"path":"density-based-clustering-algorithm.html","id":"introduction-2","chapter":"4 Density-based Clustering Algorithm","heading":"4.1 Introduction","text":"Terrestrial laser scanning (TLS) one successful methods 3D data collection last years.\nSequential acquisition can used detect quantify surface changes.\nthree main challenges arising TLS data collection : 1) large number points acquired computationally intense datasets filtered depending aim investigation; 2) data collection methods suffer perspective effects, can lead either zones occlusion (shadow effect) spatially-variable point densities; 3) 3D point clouds normally interpolated digital elevation models either regular grids triangulated irregular networks.\nThus, change detection proposes (.e. determination topographic change, including erosion deposition), appropriate develop methods based upon direct analysis point clouds using semi-automatic approaches allowing detect extract individual features.practical computer lab introduce semi-automated method developed isolating identifying features topographic change (.e., apparent changes caused surface displacements indicating erosion deposition) directly point cloud data using Density-Based Spatial Clustering Applications Noise (DBSCAN).\nmethodology developed Natan Micheletti, Marj Tonini, Stuart N. Lane8 active rock glacier front located Swiss Alps: Tsarmine rock glacier.\nFigure 2.1: Tsarmine rock glacier, Hérens Valley, Western Swiss Alps. Source: Micheletti et al, 2016\n","code":""},{"path":"density-based-clustering-algorithm.html","id":"the-overall-methodology-1","chapter":"4 Density-based Clustering Algorithm","heading":"4.2 The overall methodology","text":"Stepwise analysis:Point clouds generated using TLS number dates.Point clouds co-registered using stable zones within surveyed area.Using threshold value, points may topographical change selected.final dataset treated DBSCAN, aiming grouping cluster-points single features filtering noise points, found low-density regions.present lab deals steps 2 3: feature detection using DBSCAN.\ndetected features finally labeled clusters visualized 3D map.Material loss gain can analysed GIS environment according elevation assignment change volume change computed cluster triangulation process (performed lab).","code":""},{"path":"density-based-clustering-algorithm.html","id":"dbscan-3-d-density-based-clustering-algorithm","chapter":"4 Density-based Clustering Algorithm","heading":"4.2.1 DBSCAN: 3-D density based clustering algorithm","text":"DBSCAN allows identification spatial clusters arbitrary shape base local density points.\nPoints close together grouped cluster, isolated points labelled noise.Two parameters required perform classification: minimum number points necessary form cluster (\\(MinPts\\)), neighborhood size epsilon (\\(eps\\)).\nalgorithm explores point dataset, counting number neighboring points falling within circle (2D model) sphere (3D model) radius equal \\(eps\\) ().\nnumber equal greater \\(MinPts\\), points labelled belonging cluster (b).\nnumber lower \\(MinPts\\), points classified noise.\ncentral point cluster called core-point.\nSince points can density-reachable one core-point, can belong one cluster.\ncase clusters blended together form unique cluster feature arbitrary shape (c).\nFigure 4.1: Parameters DBSCAN form cluster\n","code":""},{"path":"density-based-clustering-algorithm.html","id":"field-campaign","chapter":"4 Density-based Clustering Algorithm","heading":"4.2.2 Field campaign","text":"ultra-long range LiDAR RIEGL VZ-6000 scanner employed acquire sequential 3D datasets rock glacier front.\nTLS scans performed different dates two consecutive summers: first survey carried 23th September 2014, last one 22th September 2015.instrument equipped -board inclination sensors: allows use inclination data obtain (locally) georeferenced datasets Z dimension represents elevation X-Y plane.\nco-registration, mask used restrict point clouds area interest: front rock glacier corridor .DBSCAN requires input 3-dimensional dataset, case consists points clouds (X,Y,Z) plus displacement distance.\ntwo co-registered datasets (2014 2015), set first target recent reference.\npoint target cloud corresponding nearest point reference cloud identified distance evaluated using software Cloud Compare.\nCo-registration errors, estimated noise real material loss/gain signals, removed analysis.\n, noted field observations size displaced boulders typically > 0.30 m use change criteria remove noise.","code":""},{"path":"density-based-clustering-algorithm.html","id":"dbscan-feature-detection-from-points-clouds","chapter":"4 Density-based Clustering Algorithm","heading":"4.3 DBSCAN: feature detection from points clouds","text":"","code":""},{"path":"density-based-clustering-algorithm.html","id":"load-the-libraries-2","chapter":"4 Density-based Clustering Algorithm","heading":"4.3.1 Load the libraries","text":"Fist load following libraries: - dbscan: fast implementation several density-based algorithms DBSCAN family.\n- rgl: Provides medium high level functions 3D interactive graphics - plot3Drgl: Plot 3D graphs rgl window.\n- classInt: Selected methods choose class intervals mapping puposes.\n- RColorBrewer: Provides color schemes mapping.","code":"\n\nlibrary(\"dbscan\")\nlibrary(\"rgl\")\nlibrary(\"plot3Drgl\")\nlibrary(\"classInt\")\nlibrary(\"RColorBrewer\")\n\n(.packages())\n#>  [1] \"RColorBrewer\" \"classInt\"     \"plot3Drgl\"   \n#>  [4] \"plot3D\"       \"rgl\"          \"dbscan\"      \n#>  [7] \"stats\"        \"graphics\"     \"grDevices\"   \n#> [10] \"utils\"        \"datasets\"     \"methods\"     \n#> [13] \"base\""},{"path":"density-based-clustering-algorithm.html","id":"import-and-visualise-the-point-clouds-dataset","chapter":"4 Density-based Clustering Algorithm","heading":"4.3.2 Import and visualise the point clouds dataset","text":"provide dataset corresponding one-year displacement distance, masked rock glacier front.\nnoise-points removed using threshold 30 cm.\nFinally plot 3D-points cloud filtered dataset using displacement distance attribute display map.","code":"\n# Import point cloud dataset (ptc)\n# 1 year displacement TLS campaigns; data masked over the active front only.\nptc <- read.table(\"data/Lab03/TsarmineRG_230914_frontonly_XYZ_dist_ref_220915.txt\", header=FALSE, sep=\"\")\n\n# Add names to columns: X,Y, Z coordinates and the displacement distance \"d\".\ncolnames(ptc)<-c(\"Y\",\"X\",\"Z\",\"d\") # rename columns\n\n# Inspect the dataset:\nstr(ptc)\nsummary(ptc$d)\nhist(log10(ptc$d))\n# Create a subset: removing noise-points (d>30cm).\nptc30 <- subset(ptc, d>=0.3)\n\n# Inspect the subset:\nstr(ptc30)\nsummary(ptc30$d)\n\n# Plot-3D: display 3D plot (X,Y,Z) with class intervals based on the distance d.\n\n# Create a class interval (int) based on natural breaks:\nint <- classIntervals(ptc30$d, n=5, style=\"fisher\") \n  cut.vals<-int$brks\n  d.cut <- cut(ptc30$d,cut.vals)\n  cut.levels <- levels(d.cut)\n  cut.band <- match(d.cut,cut.levels)\n  cls <- rev(brewer.pal(length(cut.levels), \"RdYlGn\"))\n  \n# Display the 3D-plot:\nscatter3D(ptc30$X, ptc30$Y, ptc30$Z, colvar =ptc30$d, breaks=cut.vals, cex=0.5)\nplotrgl() #animated 3D-plot"},{"path":"density-based-clustering-algorithm.html","id":"run-dbscan","chapter":"4 Density-based Clustering Algorithm","heading":"4.3.3 Run DBSCAN","text":"two parameters \\(eps\\) \\(MinPts\\) greatly affect output cluster detection leading identification large number small clusters (small values) small number large features arbitrary shape (large values).\nfixed \\(MinPts\\) parameter, suitable value \\(eps\\) neighborhood size can estimated using k-nearest neighbors (k-NN) distance graph, imposing k equal \\(MinPts\\) plotting distance nearest neighbors.\noptimal \\(eps\\)-value coincide stronger curvature curve.following, minimum number points allowing form cluster fixed first, plot k-nearest neighbor (k-NN) distance used find suitable value \\(eps\\) neighborhood size.\ncan use function find NN-distance (given k-value) corresponding stronger curvature.\nvalues gives best neighborhood size \\(eps\\)best \\(MinPts\\) adnd \\(eps\\) parameter selected, DBSCAN function can run.Finally can export result .txt file import GIS system analyses, determine volumes loss gain material.","code":"\n\n# Plot the k-NN distance graph (with k=10)\nkNNdistplot(ptc30[,-4], k = 10)\n\n# Eps=1m (~3xsigma) ; MinPts=10\ncl30<-dbscan(ptc30, 1, minPts=10)\n\n# Inspect the results:\nstr(cl30)\nsummary(cl30$cluster)\nsum(cl30$cluster==0)\n\n# Plot only the detected clusters.\n\n# Join the data:\ncl30<-cbind(ptc30, cluster=cl30$cluster)\n# Save as data frame and remove the noise (label==0)\ncl30_df<-as.data.frame(subset(cl30, cl30$cluster>=1))\n# Simple plot:\nplot(cl30_df$X, cl30_df$Y, col=cl30_df$cluster, pch=20)\n# Animated 3D-plot:\nplot3d(cl30_df, col=cl30_df$cluster, pch=20)\nwrite.table(cl30_df, file=\"cls30_1_10.txt\", sep=\"\\t\")"},{"path":"density-based-clustering-algorithm.html","id":"conclusions-and-further-analyses-2","chapter":"4 Density-based Clustering Algorithm","heading":"4.4 Conclusions and further analyses","text":"proposed method permitted detection features changes rock glacier front located Swiss Alps.\nSingle cluster features erosion deposition/front movements extracted directly point clouds, detected DBSCAN without necessity interpolate 3-D original data.sure everything perfectly clear , propose lab , changing values MinPts eps discuss compare resulting extracted features.","code":""},{"path":"self-organizing-map.html","id":"self-organizing-map","chapter":"5 Self-organizing Map","heading":"5 Self-organizing Map","text":"","code":""},{"path":"self-organizing-map.html","id":"introduction-3","chapter":"5 Self-organizing Map","heading":"5.1 Introduction","text":"increasing availability multivariate datatets stimulates researches develop new techniques differ utilized earlier scientific paradigms.\nfield social economic science, geo-demographic segmentation defines technique used classify population based input data describing administrative units people living .9\nSmall areas can thus classified discrete categories using demographic input data, means data reduction techniques (Principal component analysis).\nmain limitation approaches problem communicating multidimensional complexity characterizing different output classes.Machine learning techniques can help context, designed extract useful formation insight interaction variables describing complex structure given phenomenon.\npresent computing lab introduce unsupervised learning procedure based Self-Organizing Map (SOM),10 allowing detecting clusters characterizing pattern population dynamic Switzerland recent period.\nlab inspired work Marj Tonini et al.11","code":""},{"path":"self-organizing-map.html","id":"material-and-method","chapter":"5 Self-organizing Map","heading":"5.2 Material and method","text":"","code":""},{"path":"self-organizing-map.html","id":"the-geo-demographic-context","chapter":"5 Self-organizing Map","heading":"5.2.1 The geo-demographic context","text":"Switzerland highest life expectancy world.\ncounts 8,5 million inhabitants (official census data 2020), twice much beginning 20th century, mainly high level immigration.\nnumber foreigners currently reside country corresponds one quarter total population.\npopulation (85%) lives cities.\nPopulation aging increased course 20th century, one five person retirement age today.\npresent study, developed machine learning based approach understand describe population patterns different geographic areas.","code":""},{"path":"self-organizing-map.html","id":"som","chapter":"5 Self-organizing Map","heading":"5.2.2 SOM","text":"use SOM, unsupervised competitive learning neural network allowing representing high-dimensional feature space (defined multivariate input dataset), two-dimensional discretized pattern (SOM grid neurons).\nSOM proximity units output grid reflects similarity corresponding input observations, guarantees preservation topological structure input data.Compared approaches, SOM efficient data visualization.\nIndeed provides additional outputs heatmaps, representing distribution input feature across SOM grid, extremely useful visually explore relationship input variables.","code":""},{"path":"self-organizing-map.html","id":"swiss-census-data","chapter":"5 Self-organizing Map","heading":"5.2.3 Swiss census data","text":"Input data come national population census, provided Swiss Federal Statistical Office, including information socio-economic status population surveyed land use land cover.\ninformation aggregated municipality level purpose present investigation.\nCensus data 2020 considered analyse current pattern, former census surveys can considered assess transitions evaluate population dynamics.","code":""},{"path":"self-organizing-map.html","id":"variables-inspection","chapter":"5 Self-organizing Map","heading":"5.3 Variables inspection","text":"","code":""},{"path":"self-organizing-map.html","id":"import-the-data","chapter":"5 Self-organizing Map","heading":"5.3.1 Import the data","text":"Fist, import Swiss census dataset year 2020, referred municipality administrative units.\ncan see description selected variables , can discarded analysis.\n, following step, extract subset meaningful variables purpose present study.\nFigure 5.1: Variables description\n\nFigure 5.2: Variables description\n","code":"\n               \nknitr::include_graphics(c(\"images/Variables1.jpg\", \"images/Variables2.jpg\"))"},{"path":"self-organizing-map.html","id":"inspect-the-data","chapter":"5 Self-organizing Map","heading":"5.3.2 Inspect the data","text":"Plotting histogram variables distribution allows detect outliers, see comparable range values, evaluate data transformation necessary.","code":"\n# Plot variables' frequency distribution\nfor(i in 3:ncol(subset2020)) {      \n    hist((subset2020[ , i]),  main=colnames(subset2020[i]))\n}"},{"path":"self-organizing-map.html","id":"data-transformation","chapter":"5 Self-organizing Map","heading":"5.3.3 Data transformation","text":"Data transformation seeks make variables range comparable.\nprocess ensure data entries look similar across fields records, making information easier find, group analyze.Min-Max Scaling, data values scaled range 0 1.\nconsequence, effect outliers suppresses certain extent.\nMoreover, helps us smaller value standard deviation data scaled.","code":"\n# Custom function to implement min max scaling\nminMax <- function(x) {\n  (x - min(x)) / (max(x) - min(x))\n}\n\n# Apply max-min to the data\ndfnorm2020<- as.data.frame(lapply((subset2020[ , -c(1,2)]), minMax))\nsummary(dfnorm2020)\n# Plot the histograms of the transformed data \nfor(i in 1:ncol(dfnorm2020)) {      \n    hist((dfnorm2020[ , i]),  main=colnames(dfnorm2020[i]))\n}"},{"path":"self-organizing-map.html","id":"running-som","chapter":"5 Self-organizing Map","heading":"5.4 Running SOM","text":"","code":""},{"path":"self-organizing-map.html","id":"load-the-libraries-3","chapter":"5 Self-organizing Map","heading":"5.4.1 Load the libraries","text":"Fist load following libraries:kohonen: Supervised Unsupervised Self-Organizing Maps (SOM)kohonen: Supervised Unsupervised Self-Organizing Maps (SOM)aweSOM: offers set tools explore analyze datasets SOMaweSOM: offers set tools explore analyze datasets SOMggplot2: create Elegant Data Visualizations Using Grammar Graphicsggplot2: create Elegant Data Visualizations Using Grammar Graphicscolorpatch: rendering methods (ggplot extensions)colorpatch: rendering methods (ggplot extensions)","code":"\nlibrary(kohonen)\nlibrary(aweSOM)\nlibrary(ggplot2)\nlibrary(colorpatch)\n(.packages())"},{"path":"self-organizing-map.html","id":"compute-som","chapter":"5 Self-organizing Map","heading":"5.4.2 Compute SOM","text":"main idea SOM map input high-dimensional feature space lower-dimensional output space organized grid made regular units (.e. neurons).\nend process, observation input space (\\(X_{k,}\\)) associated (.e., mapped) unit SOM grid.\nDepending size grid, one unit can include several input observations.first step consists defining size geometry SOM grid.\ncase define rectangular grid 18 13 units, allowing allocate, average, 15 input-items per units, whose geometry reproduces shape study area.\n(NB: several parameters configurations SOM grid can implemented compared, seeking minimize Quantization Error, QE)Finally, can run SOM model.\nNote rlen indicates number times complete data set presented network, parameters keep default values.overall computation include following steps:1) data frame transformed matrix2) Create SOM-gridBefore running SOM, create grid output units.3) Run-SOMThe general R function used creating simulations random objects can reproduced.4) SOM-model evaluationFinally, can optimize size grid inspecting several quality measure changing parameters accordingly.\nparticular explore following:Changes: shows mean distance closest codebook vector training.Changes: shows mean distance closest codebook vector training.Quantization error: average squared distance data points map’s codebook mapped.\nLower better.Quantization error: average squared distance data points map’s codebook mapped.\nLower better.Percentage explained variance: similar clustering methods, share total variance explained clustering (equal 1 minus ratio quantization error total variance).\nHigher better.Percentage explained variance: similar clustering methods, share total variance explained clustering (equal 1 minus ratio quantization error total variance).\nHigher better.","code":"\nmxM2020<-as.matrix(dfnorm2020) # max-min\n# Gird size 18x13 units\nsom_grid <- somgrid(xdim = 18, ydim=13) \n# Use max-min data transformation\nset.seed(123) \n\n# Run SOM\nSOM2020M<- som(mxM2020,\n               grid=som_grid, \n               rlen=1000)\nprint(SOM2020M)\n#> SOM of size 18x13 with a rectangular topology.\n#> Training data included.\n# Evaluate rlen\nplot(SOM2020M, type=\"changes\")\n\n# Evaluate the results\nQEM<-somQuality(SOM2020M, dfnorm2020) \n\n## Quality measures:\nQEM$err.quant # Quantization error\n#> [1] 0.04534672\nQEM$err.varratio # % explained variance\n#> [1] 82.06"},{"path":"self-organizing-map.html","id":"soms-main-outputs","chapter":"5 Self-organizing Map","heading":"5.5 SOM’s main outputs","text":"main graphical outputs SOM node counts, neighborhood distances, heatmaps.Node counts map informs number input vectors falling inside output unit.Node counts map informs number input vectors falling inside output unit.Neighbourhood distance plot shows distance unit neighborhoods.Neighbourhood distance plot shows distance unit neighborhoods.Heatmaps show distribution input variable, associated input vectors, across SOM grid.Heatmaps show distribution input variable, associated input vectors, across SOM grid.Visualized side side, heatmaps provide useful tool explore correlation input variables .\nFigure 5.3: Heatmaps\n","code":"\n# Create a color palette\ncoolBlueHotRed <- function(n, alpha = 1) {rainbow(n, end=4/6, alpha=alpha)[n:1]}\n\npar(mfrow = c(1, 2))\n\n# Plot node counts map\nplot(SOM2020M, type=\"count\", main=\"Node Counts\", palette.name=coolBlueHotRed)\n\n# Plot SOM neighbourhood distance\nplot(SOM2020M, type=\"dist.neighbours\", main = \"SOM neighbour distances\")\n# Plot heatmaps for selected variables\nfor (i in 1:18) \n  {\nplot(SOM2020M, type = \"property\", property = getCodes(SOM2020M)[,i],\n     main=colnames(getCodes(SOM2020M))[i], palette.name=coolBlueHotRed)\n  }\nknitr::include_graphics(\"images/Heatmaps.jpg\")"},{"path":"self-organizing-map.html","id":"hierarchical-clustering","chapter":"5 Self-organizing Map","heading":"5.6 Hierarchical clustering","text":"Hierarchical clustering can finally performed isolate groups input vectors characterized similar distribution input variables.\nsecond step takes SOM-units (.e., CodeBook vectors) input group desired number clusters (choose 7 case).\nallows characterize main typologies geo-demographic groups.Apply hierarchical clustering:Display SOM-grid different colors cluster:Assign cluster number (based hierarchical clustering) unit:Map resulting clusters geographical spaceYou can import final table values population census variable GIS join values administrative limits municipality levels Switzerland.\nallows elaborate map .NB. need two columns: code identifying municipality (“BFS_nummer”) cluster number (hc).\nOpen table including resulting clusters dedicated program (like Excel) reorganize header, new column unique identifier created automatically (can name “ID”), headers shifted one place right.\nFigure 5.4: Spatial pattern distribution Swiss poputaion census\nwant try primary automatic process Rstudio, can conduct codes see map.","code":"\nCB2020M <- getCodes(SOM2020M) # Extract codebook vectors\n\ncls2020M <- cutree(hclust(dist(CB2020M)), 7)\n\n# Colour palette definition, will use later too.\n# If you change the number of hierarchical clustering, remember change the number of color.\n\nmap_palette <- c(\"steelblue3\",\"darkgoldenrod4\",\"darkolivegreen\", \"springgreen3\", \"darkorange\", \"firebrick\", \"darkolivegreen1\" )\n\nplot(SOM2020M, type=\"mapping\", pchs=\"\", bgcol = map_palette[cls2020M], main = \"Clusters\")\nclasgn <- cls2020M[SOM2020M$unit.classif]\ndfnorm2020$hc<-clasgn\nclsnorm<-cbind(dfnorm2020, subset2020$BFS_nummer)\n\n# Export the table with clusters \nwrite.table(clsnorm, file=\"hcM_7cls.csv\", sep=\",\")\nlibrary(st)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(ggspatial)\nlibrary(dplyr)\n# Load the shapefile data\nCH_outline <- st_read(\"data/Lab04/Municiplities.shp\")\n\n# Quick check the shapefile data\nggplot() + \n  geom_sf(data = CH_outline, size = 1.5, color = \"black\", fill = \"cyan1\") + \n  ggtitle(\"Swiss municiplities\") + \n  coord_sf()\n\n# Rename the column\ncolnames(clsnorm)[20] <- \"BFS_NUMMER\"\n\n# Merge the dataframe and shapefile together by the column with the same name\nz = merge(CH_outline, clsnorm, by = \"BFS_NUMMER\")\n\n# Rename the column with clustering results\nz$cluster <- as.numeric(z$hc)\n# Mapping the clusters via ggplot like QGIS/ArcGIS Pro/Arcmap, similar package 'tmap'\nCH_cluster <- ggplot(data = z) + # Original data\n    geom_sf(aes(fill = as.factor(cluster))) + # Mapping the clusters\n    annotation_scale(location = \"bl\", width_hint = 0.3,\n                     pad_x = unit(0.2, \"cm\"), pad_y = unit(0.1, \"cm\")) + # Mapping scales, which are calculated by project coordinates\n    annotation_north_arrow(location = \"tr\", which_north = \"true\", \n          pad_x = unit(0.0, \"cm\"), pad_y = unit(0.1, \"cm\"),\n          style = north_arrow_fancy_orienteering) + # Mapping north arrow\n    scale_colour_manual(values = map_palette) + # Set up the our color palette\n    theme_minimal() + # Background color, if use, it means without color\n    labs(title = \"Spatial pattern distribution of Swiss poputaion census\") + # Insert title of the map\n    scale_fill_discrete(name = \"Clusters\", \n                        labels = c(\"1: Unproductive\", \"2: Forest/Peri-urban\", \"3: Peri-urban/Rural\",\n                                 \"4: Rural\",\"5: Suburban\",\"6: Urban centre\",\"7: Peri-urban fringe\")) # Legend labels\n    guides(fill=guide_legend(title=\"Clusters\")) # Legend name\n\n# Save the map as jpg\nggsave(\"CH_cluster.jpg\")\n    \nCH_cluster"},{"path":"self-organizing-map.html","id":"clusters-characterization","chapter":"5 Self-organizing Map","heading":"5.6.1 Clusters characterization","text":"interpret final main clusters therms geo-demographic characteristics, can evaluate distribution variable within clusters using box plots (also known whisker plot).Box-plot standardized way display dataset based five-number summary statistics: minimum, maximum, sample median, first third quartiles (.e., median lower half (25%) median upper half (75%) dataset).better investigate values assumed class variables withing different clusters, can group category.\nFigure 5.5: Physical space\n\nFigure 5.6: Demographics\n\nFigure 5.7: Socio-economics\n","code":"\n\n# Creates an empty list object that will be filled by the loop\ncls20M <- list() \n\n# Split the single clusters\nfor(i in 1:7) {\n  cls20M[[i]]<-subset(clsnorm, clsnorm$hc==i) \n}\n\nclsvar20M <- lapply(cls20M, \"[\", c(1:18)) \n\n# Box-plots for the single clusters\nfor (i in 1:7) {boxplot ((clsvar20M[[i]]), main=paste(\"Cluster\", i), mar=c(8,3,3,1), cex.axis=0.5,  las=2)}\n\n# Box plot by categories: \"Physical space\"\n\nclsvar20M <- lapply(cls20M, \"[\", c(1:5)) \n\npar(mfrow=c(2,4), mar=c(7,3,3,1), cex.axis=0.7)\n\nfor (i in 1:7) {boxplot ((clsvar20M[[i]]), main=paste(\"Cluster\", i),  las=2)}\n\n# Box plot by categories: \"Demographics\"\n\nclsvar20M <- lapply(cls20M, \"[\", c(6:13)) \n\npar(mfrow=c(2,4), mar=c(7,3,3,1), cex.axis=0.7)\n\nfor (i in 1:7) {boxplot ((clsvar20M[[i]]), main=paste(\"Cluster\", i),  las=2)}\n\n# Box plot by categories: \"Socio-economics\"\n\nclsvar20M <- lapply(cls20M, \"[\", c(14:18)) \n\npar(mfrow=c(2,4), mar=c(7,3,3,1), cex.axis=0.7)\n\nfor (i in 1:7) {boxplot ((clsvar20M[[i]]), main=paste(\"Cluster\", i),  las=2)}"},{"path":"self-organizing-map.html","id":"conclusions","chapter":"5 Self-organizing Map","heading":"5.7 Conclusions","text":"Results present study reveal main patterns population Switzerland based surveyed land-use, socio-economic, demographic indicators.\ncharacterize final main clusters, distribution variable within assessed using box plots.\nthus identify main clusters including developed active cities higher income, peri-urban areas mostly devoted agricultural activity, areas higher levels migration.SOM heatmaps allow display pattern distribution input variable SOM-grid values change space.\nVisualized side side, heatmaps show picture different areas characteristics.\nway possible explore level complementarity links one variables among .conclusion, present study proposed performant data-driven approach based unsupervised learning allowing extract useful information huge volume multivariate population census data.\napproach led represent interpret main patterns characterizing dynamic population Switzerland recent period.","code":""},{"path":"self-organizing-map.html","id":"further-analyses","chapter":"5 Self-organizing Map","heading":"5.8 Further analyses","text":"sure everything perfectly clear , propose answer following questions discuss answers participants course directly teacher.Change size gridmap check get better results SOM. N.B. Better results achieved quantisation error decreases, explained variance increases, empty observations revealed Node Counts map.2.1) Focusing Cluster 6: variable characterize ?\nBased variables, class land use can associated cluster?\n2.2) case Cluster 4?Describe distribution clusters geographical space.\ndetails, describe kind land use different clusters can referred specify .Describe distribution clusters geographical space.\ndetails, describe kind land use different clusters can referred specify .visual inspection heatmaps, describe correlation can observe following variables: ) “p_transport” “p_infrastructure”; b) “p_agriculture” “p_improductible”.visual inspection heatmaps, describe correlation can observe following variables: ) “p_transport” “p_infrastructure”; b) “p_agriculture” “p_improductible”.","code":""},{"path":"random-forest.html","id":"random-forest","chapter":"6 Random Forest","heading":"6 Random Forest","text":"","code":""},{"path":"random-forest.html","id":"introduction-4","chapter":"6 Random Forest","heading":"6.1 Introduction","text":"application, explore capabilities stochastic approach based machine learning (ML) algorithm elaborate landslides susceptibility mapping Canton Vaud, Switzerland.\nGenerally speaking, ML includes class algorithms analysis, modelling, visualization environmental data performs particularly well model environmental hazards, naturally complex non-linear behavior.\nuse Random Forest, ensemble ML algorithm based decision trees.research framework inspired computational lab refers pioneering study susceptibility mapping wildfire eventy Marj Tonini et al.12 developed assessment variable importance Andrea Trucchia et al.13","code":""},{"path":"random-forest.html","id":"the-main-objective","chapter":"6 Random Forest","heading":"6.1.1 The main objective","text":"Landslides one major hazard occurring around world.\nSwitzerland, landslides cause damages infrastructures sometimes threaten human lives.\nShallow landslides triggered intense rainfalls.\nslope movements generally rapid hardly predictable.\nDifferent quantitative approaches developed assess susceptible areas.project applies data-driven methodology based Random Forest (RF) (Leo Breiman14) elaborate landslides susceptibility map canton Vaud, Switzerland.\nRF applied set independent variables (.e., predictors) dependent variables (inventoried landslides equal number locations absences).\noverall methodology described following graphic ().\nFigure 6.1: Basic elements generic methodology\n","code":""},{"path":"random-forest.html","id":"load-the-libraries-4","chapter":"6 Random Forest","heading":"6.1.2 Load the libraries","text":"perform analysis, first install following libraries:library(raster): raster package provides classes functions manipulate geographic (spatial) data ‘raster’ format.library(raster): raster package provides classes functions manipulate geographic (spatial) data ‘raster’ format.library(readr): goal ‘readr’ provide fast friendly way read rectangular data (like ‘csv’, ‘tsv’, ‘fwf’).library(readr): goal ‘readr’ provide fast friendly way read rectangular data (like ‘csv’, ‘tsv’, ‘fwf’).library(randomForest): Classification regression based forest trees using random inputs, based Breiman (2001).library(randomForest): Classification regression based forest trees using random inputs, based Breiman (2001).library(dplyr): next iteration plyr, focused tools working data frames (hence d name).library(dplyr): next iteration plyr, focused tools working data frames (hence d name).library(pROC): Allowing compute, analyze ROC curves, \nlibrary(plotROC) display ROC curve\nlibrary(pROC): Allowing compute, analyze ROC curves, andlibrary(plotROC) display ROC curve(ggplot2): system declaratively creating graphics.(ggplot2): system declaratively creating graphics.library(sf): Support simple features, standardized way encode spatial vector data.library(sf): Support simple features, standardized way encode spatial vector data.List libraries","code":"\nlibrary(raster) \nlibrary(readr) \nlibrary(randomForest)\nlibrary(dplyr) \nlibrary(pROC) \nlibrary(plotROC) \nlibrary(ggplot2)  \nlibrary(sf) \nlibrary(classInt)\n\n(.packages())\n#>  [1] \"classInt\"     \"sf\"           \"plotROC\"     \n#>  [4] \"ggplot2\"      \"pROC\"         \"dplyr\"       \n#>  [7] \"randomForest\" \"readr\"        \"raster\"      \n#> [10] \"sp\"           \"stats\"        \"graphics\"    \n#> [13] \"grDevices\"    \"utils\"        \"datasets\"    \n#> [16] \"methods\"      \"base\""},{"path":"random-forest.html","id":"import-the-data-1","chapter":"6 Random Forest","heading":"6.2 Import the data","text":"Import landslides punctual dataset presences pseudo-absences (LS_pa) predictors (raster format).\nhelp exploratory data analyses steps understand input data structure.","code":""},{"path":"random-forest.html","id":"landslides-dataset","chapter":"6 Random Forest","heading":"6.2.1 Landslides dataset","text":"landslide inventory provided environmental office canton Vaud.\nshallow landslides used susceptibility modelling.\nOne pixel per landslide-area (namely one located highest elevation) extracted.\nSince landslide scarp located upper part polygon, makes sense consider highest pixel characterize single event.model includes implementation landslide pseudo-absences, areas hazardous events took place (.e. landslide location known mapped footprint areas available, non-landslide areas defined).\nIndeed, assure good generalization model avoid overestimation absence, pseudo-absences need generated cases explicitly expressed.\ncase study, equal number point presences randomly generated study area, except within landslides polygons, lakes glaciers (called “validity domain”, events potentially occur).","code":"\n\n# Import the landslides dataset (dependent variable)\nLS_pa <- read.csv(\"data/Lab05/LS_pa.csv\") \n\n# Convert the numeric values (0/1) as factor \n##(i.e. categorical value)\nLS_pa$LS<-as.factor(LS_pa$LS)\n\n# Display the structure (str) and result summaries (summary)\nstr(LS_pa)\nsummary(LS_pa)\n# Plot the events\nplot(LS_pa$X,LS_pa$Y, col=LS_pa$LS, pch=20, cex=0.7)"},{"path":"random-forest.html","id":"predictor-variables","chapter":"6 Random Forest","heading":"6.2.2 Predictor variables","text":"Selecting predictor variables key stage susceptibility ans risk modelling using data-driven approach.\nconsensus number variables variables used landslides assesment.\npresent exercise use following:DEM (digital elevation model): provided Swiss Federal Office Topography.\nelevation direct conditioning factor landslide; however, can reflect differences vegetation characteristics soil.DEM (digital elevation model): provided Swiss Federal Office Topography.\nelevation direct conditioning factor landslide; however, can reflect differences vegetation characteristics soil.Slope: one explicating factor landslide susceptibility modelling.\ncomputing :Slope: one explicating factor landslide susceptibility modelling.\ncomputing :\\[Slope = arctan(\\sqrt{(dz/dx)^2 + (dz/dy)^2)} * (\\pi/2)\\]Curvature: curvature widely used landslide susceptibility modelling.\nallows assessing water flow acceleration sediment transport process (profile curvature) water flow propensity converge diverge (plan curvature).\nderived DEM using curvature tool ArcGIS.Curvature: curvature widely used landslide susceptibility modelling.\nallows assessing water flow acceleration sediment transport process (profile curvature) water flow propensity converge diverge (plan curvature).\nderived DEM using curvature tool ArcGIS.TWI (topographical water index): topography plays key role spatial distribution soil hydrological conditions.\nDefining \\(\\alpha\\) upslope contributing area describing propensity cell receive water, \\(\\beta\\) slope angle, TWI (compute formula ), reflects propensity cell evacuate water:TWI (topographical water index): topography plays key role spatial distribution soil hydrological conditions.\nDefining \\(\\alpha\\) upslope contributing area describing propensity cell receive water, \\(\\beta\\) slope angle, TWI (compute formula ), reflects propensity cell evacuate water:\\[TWI=ln(\\alpha/tan(\\beta))\\]Distance roads: roads build mountainous areas often cut slope, weakening cohesion soil.\nMoreover, roads surfaces highly impermeable.\nraster elaborated using euclidean distance tool ArcGIS, swissTLMRegio map roads represented lines.Distance roads: roads build mountainous areas often cut slope, weakening cohesion soil.\nMoreover, roads surfaces highly impermeable.\nraster elaborated using euclidean distance tool ArcGIS, swissTLMRegio map roads represented lines.Land Cover: developed Swiss administration based aerial photographs control points.\nincludes 27 categories distributed following 6 domains: human modified surfaces, herbaceous vegetation, shrubs vegetation, tree vegetation, surfaces without vegetation, water surfaces (glaciers included).Land Cover: developed Swiss administration based aerial photographs control points.\nincludes 27 categories distributed following 6 domains: human modified surfaces, herbaceous vegetation, shrubs vegetation, tree vegetation, surfaces without vegetation, water surfaces (glaciers included).Geology: use lithology increase performance susceptibility landslide models.\nuse map elaborated Canton Vaud, defining geotypes reclassified 10 classes order differentiate sedimentary rocks.Geology: use lithology increase performance susceptibility landslide models.\nuse map elaborated Canton Vaud, defining geotypes reclassified 10 classes order differentiate sedimentary rocks.predictor variables (features) aggregated.\nuse function stack create collection RasterLayer objects spatial extent resolution.","code":"\n\n## Import raster (independent variables) 25 meter resolution\n\nlandCover<-as.factor(raster(\"data/Lab05/landCover.tif\"))\ngeology<-as.factor(raster(\"data/Lab05/geology.tif\"))\nplanCurv<-raster(\"data/Lab05/plan_curvature.tif\")/100 # this because ArcGIS multiply \nprofCurv<-raster(\"data/Lab05/profil_curvature.tif\")/100 # ....curvature values by 100\nTWI<-raster(\"data/Lab05/TWI.tif\")\nSlope<-raster(\"data/Lab05/Slope.tif\")\ndem<-raster(\"data/Lab05/DEM.tif\")\ndist<-raster(\"data/Lab05/dist_roads.tif\")\n#create a Raster stack\nfeatures<-stack(dist, dem, TWI, planCurv, profCurv, Slope, geology, landCover)\n\n# Renames the variables\nnames(features)<-c(\"distRoad\", \"DEM\", \"TWI\", \"planCurv\", \"profCurv\", \n                   \"Slope\", \"Geology\", \"LandCover\")\nplot(features)"},{"path":"random-forest.html","id":"the-use-of-categorical-variables-in-machine-learning","chapter":"6 Random Forest","heading":"6.2.2.1 The use of categorical variables in Machine Learning","text":"majority ML algorithms (e.g., support vector machines, artificial neural network, deep learning) makes predictions base proximity values predictors, computed terms euclidean distance.\nmeans algorithms can handle directly categorical values (.e., qualitative descriptors).\nThus, cases, categorical variables need transformed numerical format.\nOne advantage using Random Forest (implemented R) can handle directly categorical variables, since algorithm operate constructing multitude decision trees training time best split chosen just counting proportion class observation.understand characteristics categorical variables, can plot tow rasters Land Cover Geology using original classes.\nvisualize data, can use R GIS, need perform data manipulations.Raster geology: extract attribute table plot map based geological classes.Raster land cover: extract attribute table plot map based land cover classes.","code":"\n\n# Create a random color palette \nlibrary(RColorBrewer)\nn <- 50\nqual_col_pals = brewer.pal.info[brewer.pal.info$category == 'qual',]\ncol_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, \n                           rownames(qual_col_pals)))\n\n# Load libraries for levelplot function, \n# allowing to plot raster based of categorical data.\nlibrary(lattice)\nlibrary(rasterVis)\n\n# ensure that the raster attributes are read as type \"factor\"\ngeology <- as.factor(raster(\"data/Lab05/geology.tif\"))\n\n# Add a geological class column to the Raster Attribute Table\nrat <- levels(geology)[[1]] \nrat$Geology_classes <- c(\"glacial deposits\", \"carbonates\", \"marly-limestone\",\n                         \"alluvial deposits\", \"artificial materials\", \n                         \"slope deposits\", \"detrital rocks\", \"metamorphic rocks\",\n                         \"evaporitic rocks\",\"magmatic rocks\")\nlevels(geology) <- rat\nlevels(geology)\n\n# Plot the raster based on the geological classes \nlevelplot(geology, col.regions=col_vector)\n\nlandCover <- as.factor(raster(\"data/Lab05/landCover.tif\"))\nratLC <- levels(landCover)[[1]]  \n\nratLC$LandCover_classes <- c(\"impermeable man-made\",\"permeable man-made\",\n                             \"herbaceous vegetation\",\"shrub vegetation\",\n                             \"forest\",\"no vegetation\",\"glacier and water body\")\nlevels(landCover) <- ratLC\nlevels(landCover)\n\n# Plot the raster based on the land cover classes \nlevelplot(landCover, col.regions=col_vector)"},{"path":"random-forest.html","id":"create-the-input-dataset","chapter":"6 Random Forest","heading":"6.3 Create the input dataset","text":"step, extract values predictors location landslides (presences absences) dataset.","code":"\n\n# Shuffle the rows\nLS_sh <- LS_pa [sample(nrow(LS_pa), nrow(LS_pa)), ]\n\n# Convert to spatial point dataframe\nLS_spdf<-SpatialPointsDataFrame(LS_sh[,c(\"X\", \"Y\")], LS_sh, proj4string=crs(features))\n\nLS_input <- extract(features, LS_spdf, df=TRUE)\n\n# Add independent variable (LS) as additional column to the dataframe\nLS_input$LS <- as.factor(LS_spdf$LS)\n\n# Convert Land cover and Geology classes as factor\nLS_input$LandCover <- as.factor(LS_input$LandCover)\nLS_input$Geology <- as.factor(LS_input$Geology)\n\n# Remove extra column (ID)\nLS_input <- LS_input[,2:ncol(LS_input)]\nLS_input <- na.omit(LS_input)\n\n# Explore the newly created input dataset.\nhead(LS_input)\nstr(LS_input)"},{"path":"random-forest.html","id":"split-the-input-dataset-into-training-80-and-testing-20","chapter":"6 Random Forest","heading":"6.3.1 Split the input dataset into training (80%) and testing (20%)","text":"well-established procedure ML split input dataset training, validation, testing.training dataset needed calibrate parameters model, used get predictions new data.training dataset needed calibrate parameters model, used get predictions new data.purpose validation dataset optimize hyperparameter model (training phase).\nNB: Random Forest subset represented --Bag (OOB)!purpose validation dataset optimize hyperparameter model (training phase).\nNB: Random Forest subset represented --Bag (OOB)!provide unbiased evaluation final model assess performance, results predicted unused observations (prediction phase), defined testing dataset.provide unbiased evaluation final model assess performance, results predicted unused observations (prediction phase), defined testing dataset.","code":"\n\n# Split the input dataset into training (80%) and testing (20%)\nn <- nrow (LS_input)\nset.seed(123)\nn_train <- round(0.80 * n) \ntrain_indices <- sample(1:n, n_train)\n\n# Create indices\nLS_input_train <- LS_input[train_indices, ]  \nLS_input_test <- LS_input[-train_indices, ]\n\n# Count the number of elements in the two subset: training and testing\ncount(LS_input_train)\ncount(LS_input_test)"},{"path":"random-forest.html","id":"run-random-forest","chapter":"6 Random Forest","heading":"6.4 Run Random Forest","text":"Computationally, subset training dataset generated bootstrapping (.e. random sampling replacement).\nsubset decision tree grown , split, algorithm randomly selects number variables (mtry) computes Gini index identify best one.\nprocess stops node contains less fixed number data points.\nfundamental hyperparameters needs defined RF mtry total number trees (ntrees).prediction error training dataset finally assessed evaluating predictions observations used subset, defined “--bag” (OOB).\nvalues used optimize values hyperparameters, trial error process (, trying minimize OOB estimate error rate).","code":"\n  \n# Set the seed of R‘s random number generator, \n## this is useful for creating simulations that can be reproduced.\nset.seed(123) \n\n# Run RF model\nRF_LS<-randomForest(y=LS_input_train$LS, x=LS_input_train[1:8],data=LS_input_train,\n                    ntree=200, mtry=3,importance=TRUE, do.trace=TRUE)"},{"path":"random-forest.html","id":"rf-main-outputs","chapter":"6 Random Forest","heading":"6.4.1 RF main outputs","text":"Printing results RF allows gain insight outputs implemented model, namelly following: - summary model hyperparameters - OOB estimate error rate - confusion matrix; case 2x2 matrix used evaluating performance classification model (1-presence vs 0-absence).plotting error rate useful estimate decreasing values OOB predictions (presence (1) / absence (0)) increasing number treees.","code":"\nprint(RF_LS)\n#> \n#> Call:\n#>  randomForest(x = LS_input_train[1:8], y = LS_input_train$LS,      ntree = 200, mtry = 3, importance = TRUE, do.trace = TRUE,      data = LS_input_train) \n#>                Type of random forest: classification\n#>                      Number of trees: 200\n#> No. of variables tried at each split: 3\n#> \n#>         OOB estimate of  error rate: 15.98%\n#> Confusion matrix:\n#>      0    1 class.error\n#> 0 1685  385   0.1859903\n#> 1  278 1800   0.1337825\n\nplot(RF_LS)\nlegend(x=\"topright\", legend=c(\"perd 0\", \"pred 1\", \"OOB error\"), \n col=c(\"red\", \"green\", \"black\"), lty=1:2, cex=0.8)\n# Show the predicted probability values\nRF.predict <- predict(RF_LS,type=\"prob\")\nhead(RF.predict) # 0==abstece ; 1== presence\n#>               0          1\n#> 2465 0.96428571 0.03571429\n#> 2513 0.67073171 0.32926829\n#> 2229 0.02739726 0.97260274\n#> 527  0.03896104 0.96103896\n#> 4294 0.53623188 0.46376812\n#> 2988 0.52307692 0.47692308"},{"path":"random-forest.html","id":"model-evaluation","chapter":"6 Random Forest","heading":"6.4.2 Model evaluation","text":"prediction capability implemented RF model can evaluated predicting results previously unseen data, testing dataset.\nArea “Receiver Operating Characteristic (ROC)” Curve (AUC) represents evaluation score used indicator goodness model classifying areas susceptible landslides.\nROC curve graphical technique based plot percentage correct classification (true positives rate) false positives rate (occurring outcome incorrectly predicted belonging class “1” actually belongs class “0”), evaluates many thresholds.\nAUC value lies 0.5, denoting bad classifier, 1, denoting excellent classifier.","code":"\n\n# Make predictions on the testing dataset\nRFpred_test <- predict(object = RF_LS, newdata = LS_input_test, type=\"prob\")\n \nroc <- roc(LS_input_test$LS,RFpred_test[,2])\nplot(1-roc$specificities,roc$sensitivities, type = 'l', col = 'blue',\n     xlab = \"False positive rate\", ylab = \"True positive rate\")\n\nroc "},{"path":"random-forest.html","id":"susceptibility-map","chapter":"6 Random Forest","heading":"6.5 Susceptibility map","text":"now elements elaborate final landslide susceptibility map.\nachieved making predictions (presence ) based values predictors, stored RasterStack named *features*, created .","code":"\n\n# Index=2 indicate the prediction of presence (1) \nScp_pred <- predict(features, RF_LS,type=\"prob\", index=2)\n\n# Summary statistics\nsummary(Scp_pred)\nhist(Scp_pred)\n\n# Export ratster\nwriteRaster(Scp_pred,\"Scp_map25.tif\", overwrite=TRUE)\n\n# Plot the output susceptibility map\nlibrary(\"RColorBrewer\")\nplot(Scp_pred, xlab = \"East [m]\", ylab = \"North [m]\", \n     main = \"Landslides susceptibility map\", \n     col = terrain.colors(5))"},{"path":"random-forest.html","id":"class-intervals-for-decision-maker","chapter":"6 Random Forest","heading":"6.5.1 Class intervals for decision maker","text":"can say looking map?\nActually risk heat map data visualization tool communicating level specific risk occur.\nmaps helps authorities identify prioritize risks associated given hazard.Normally authority (.e., decision maker) prioritize efforts based available resources .\n, can useful detect areas highest probability burning based given intervals (.e., breaks).\nauthority can thus concentrate resources preventive actions given percentage (5%, 10%, 20%) area highest probability burning, instead concentrate areas “stochastic” output probability value 0.8 (example).Breaks chosen based summary statics: values corresponds 0-25%-50%-75%-100% p-value distribution.Breaks chosen based well-estabished percentile classes","code":"\n\nbrk<-c(0, 0.03, 0.14, 0.4, 1) \n\nplot(Scp_pred, xlab = \"East [m]\", ylab = \"North [m]\", \n     main = \"Landslides susceptibility map\", \n     col = rev(c(\"brown\", \"orange\", \"yellow\", \"grey\")), breaks=brk)\n\n# Output predicted values are transformed to a vector\npred.vect <- as.vector(Scp_pred@data@values)\n\n# The function \"quantile\" is used to fix classes\nqtl.pred <- quantile(pred.vect,probs=c(0.25,0.5,0.75,0.85,0.95), na.rm = TRUE)\n# and then extract the corresponding values\nqtl.int<- c(0,0.03,0.14,0.42,0.6,0.82,1)\nplot(Scp_pred, xlab = \"East [m]\", ylab = \"North [m]\", \n     main = \"Landslides susceptibility map\", \n     col = rev(c(\"brown\", \"red\", \"orange\",\"yellow\", \"green\", \"grey\")), breaks=qtl.int)"},{"path":"random-forest.html","id":"variable-importance-plot","chapter":"6 Random Forest","heading":"6.6 Variable importance plot","text":"Although machine learning algorithms often considered black box, RF provides two metrics allowing assess importance variables model: mean decrease accuracy (MDA), mean decrease Gini index.\nhigher values indicate important variables.\naddition, possible plot sample tree (selected randomly) analyse structure.","code":"\n\nlibrary(\"party\")\nx <- ctree(LS~., data=LS_input_train)\nplot(x, type=\"simple\")\n# Display the plot with the relative importance of each variable\nimportance(RF_LS)\nvarImpPlot(RF_LS)"},{"path":"random-forest.html","id":"partial-dependence-plot","chapter":"6 Random Forest","heading":"6.6.1 Partial dependence plot","text":"addition, partial dependent plot allows estimate, single variable, relative probability prediction success different ranges values.\ngives graphical depiction marginal effect variable class probability different ranges continuous discrete values.\nPositive values associated probability occurrence phenomena (.e., landslides presence), negative vales indicate absence.","code":"\n\n# Slope\npartialPlot(RF_LS, LS_input_train, x.var = Slope, rug = TRUE, which.class = RF_LS$classes[2],\n            xlab= \"Slope [Â°]\", main = \"\", ylab = \"Partial dependence\")\n\n# Elevation\npartialPlot(RF_LS, LS_input_train, x.var = DEM, rug = TRUE, which.class = RF_LS$classes[2],\n            xlab= \"Elevation [m]\", main = \"\",ylab = \"Partial dependence\")\n\n# Profile curvature\npartialPlot(RF_LS, LS_input_train, x.var = profCurv, rug = TRUE, which.class = RF_LS$classes[2],\n            xlab= \"Profile curvature [1/m]\", main = \"\", ylab = \"Partial dependence\", xlim = c(-0.04,0.04))\n\n# Plan Curvature\npartialPlot(RF_LS, LS_input_train, x.var = planCurv, rug = TRUE, which.class = RF_LS$classes[2],\n            xlab= \"Plan curvature [1/m]\", main = \"\", ylab = \"Partial dependence\", xlim = c(-0.04,0.04))\n\n# Distance to roard\npartialPlot(RF_LS, LS_input_train, x.var = distRoad, rug = TRUE, which.class = RF_LS$classes[2],\n            xlab= \"Distance to road [m]\", main = \"\", ylab = \"Partial dependence\")\n\n# Topographic wetness index\npartialPlot(RF_LS, LS_input_train, x.var = TWI, rug = TRUE, which.class = RF_LS$classes[2],\n            xlab= \"TWI [-]\", main = \"\", ylab = \"Partial dependence\")\n\n# Geology\npartialPlot(RF_LS, LS_input_train, x.var = Geology, rug = TRUE, which.class = RF_LS$classes[2],\n            xlab= \"Geology\", main = \"\", ylab = \"Partial dependence\")\n\n# Land Cover\npartialPlot(RF_LS, LS_input_train, x.var = LandCover, rug = TRUE, which.class = RF_LS$classes[2],\n            xlab= \"Land Cover\", main = \"\", ylab = \"Partial dependence\")"},{"path":"random-forest.html","id":"conclusions-and-further-analyses-3","chapter":"6 Random Forest","heading":"6.7 Conclusions and further analyses","text":"exercise allowed familiarize Random Forest, proposed application landslides susceptibility mapping variables importance assessment.sure everything perfectly clear , propose answer following questions discuss answers participants course directly teacher.important implement pseudo-absence, presences (.e., observations) data-driven modelization?important implement pseudo-absence, presences (.e., observations) data-driven modelization?difference numerical categorical variable?\nGive examples types.difference numerical categorical variable?\nGive examples types.values OOB estimate error rate model?\nparameters can change try reduce ?\nbrave (.e., change values ntree mtry, analyse values AUC obtain model perform better.values OOB estimate error rate model?\nparameters can change try reduce ?\nbrave (.e., change values ntree mtry, analyse values AUC obtain model perform better.three important variables model (based MDA)?three important variables model (based MDA)?slope value (range values) gives highest probability landslides occurrence?\ngeology, important classes?slope value (range values) gives highest probability landslides occurrence?\ngeology, important classes?model seeking estimate vulnerability wildfire, dependent variable can independent variables?\nreading paper @tonini_evolution_2017, inspired lab, can help task.model seeking estimate vulnerability wildfire, dependent variable can independent variables?\nreading paper @tonini_evolution_2017, inspired lab, can help task.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
